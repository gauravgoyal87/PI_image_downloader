{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bing_image_downloader import downloader\n",
    "import os\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bing_image_downloader import downloader\n",
    "\n",
    "def download_images(request):\n",
    "\n",
    "    # “””Store downloaded images in a directory.\n",
    "\n",
    "    # Parameters:\n",
    "\n",
    "    # request: list of queries to search for\n",
    "\n",
    "        # Return: Images downloaded that match request\n",
    "\n",
    "        # “””\n",
    "\n",
    "    # print(request)\n",
    "\n",
    "    downloader.download(request, \n",
    "\n",
    "                limit=10, \n",
    "\n",
    "                output_dir='images', \n",
    "\n",
    "                adult_filter_off=True, \n",
    "\n",
    "                force_replace=False, \n",
    "\n",
    "                timeout=60,\n",
    "                \n",
    "                filter = ['photo'],\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up functions, update files in csv after removing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanup(file, files_name):\n",
    "    folder_path = f'images/{file}'\n",
    "    files = [f for f in os.listdir(folder_path) if not f.endswith('.csv')]\n",
    "    files.sort()\n",
    "\n",
    "    index = 1\n",
    "    file_changes = []\n",
    "    original_indexes = []\n",
    "    new_indexes = []\n",
    "\n",
    "    for filename in files:\n",
    "        _, ext = os.path.splitext(filename)\n",
    "        match = re.search(r'\\d+', filename)  # Find the numeric part in the file name\n",
    "        if match:\n",
    "            file_index = int(match.group())  # Extract the index\n",
    "            new_filename = f'{files_name}{index}{ext}'\n",
    "            if filename != new_filename:\n",
    "                file_changes.append([filename, new_filename])\n",
    "            original_indexes.append(file_index)  # Append the original index\n",
    "            new_indexes.append(index)\n",
    "            os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n",
    "            index += 1\n",
    "    return original_indexes, new_indexes\n",
    "\n",
    "def csv_update(file, original_indexes, new_indexes):\n",
    "    # Path to the original CSV file\n",
    "    original_csv_path = f'images/{file}/{file}.csv'\n",
    "\n",
    "    # Path to the new CSV file\n",
    "    new_csv_path = f'images/{file}/{file}_new.csv'\n",
    "\n",
    "    # Read the original CSV file and extract links corresponding to original indexes\n",
    "    links = {}\n",
    "    with open(original_csv_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            links[int(row['index'])] = row['link']\n",
    "\n",
    "    # Create a new CSV file with new indexes and corresponding links\n",
    "    with open(new_csv_path, mode='w', newline='') as new_csv_file:\n",
    "        fieldnames = ['index', 'links']\n",
    "        writer = csv.DictWriter(new_csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for original_index, new_index in zip(original_indexes, new_indexes):\n",
    "            writer.writerow({'index': new_index, 'links': links.get(original_index, '')})\n",
    "def csv_status(file, original_indexes, new_indexes,new_csv_path):\n",
    "     # Path to the original CSV file\n",
    "    original_csv_path = f'images/{file}/{file}.csv'\n",
    "\n",
    "    # Path to the new CSV file\n",
    "    # new_csv_path = f'images/{file}/{file}_new.csv'\n",
    "    # Path to the final CSV file\n",
    "    final_csv_path = f'images/{file}/{file}_final.csv'\n",
    "\n",
    "    # Read the original CSV file and extract links corresponding to original indexes\n",
    "    links = {}\n",
    "    with open(original_csv_path, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            links[int(row['index'])] = row['link']\n",
    "\n",
    "    # Create a copy of the new CSV file\n",
    "    with open(final_csv_path, mode='w', newline='') as final_csv_file:\n",
    "        fieldnames = ['index', 'links', 'status']\n",
    "        writer = csv.DictWriter(final_csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Copy content from the new CSV file\n",
    "        with open(new_csv_path, mode='r') as new_csv_file:\n",
    "            csv_reader = csv.DictReader(new_csv_file)\n",
    "            for row in csv_reader:\n",
    "                writer.writerow({'index': row['index'], 'links': row['links'], 'status': 'used'})\n",
    "        \n",
    "        # Check for links from the original CSV file not present in the new CSV file\n",
    "        for original_index, link in links.items():\n",
    "            if original_index not in original_indexes:\n",
    "                writer.writerow({'index': original_index, 'links': link, 'status': 'deleted'})\n",
    "\n",
    "    # print(f\"Final CSV file '{final_csv_path}' created successfully.\")\n",
    "def files_update(file, new_file_name):\n",
    "    original_csv_path = f'images/{file}/{file}.csv'\n",
    "    original_indexes = []\n",
    "    new_indexes = []\n",
    "    new_csv_path = f'images/{file}/{file}_new.csv'\n",
    "\n",
    "    # Clean up the folder and update file names\n",
    "    original_indexes,new_indexes  = cleanup(file, new_file_name)\n",
    "\n",
    "    # Update the CSV file with new indexes\n",
    "    csv_update(file, original_indexes,new_indexes)\n",
    "\n",
    "    # Update the status in the final CSV file\n",
    "    csv_status(file, original_indexes, new_indexes,new_csv_path)\n",
    "def capitalize_and_underscore(names):\n",
    "    renamed_names = []\n",
    "    for name in names:\n",
    "        # Split the name by space and capitalize each word\n",
    "        capitalized_words = [word.capitalize() for word in name.split()]\n",
    "        # Join the words with underscores\n",
    "        renamed_name = '_'.join(capitalized_words)\n",
    "        renamed_names.append(renamed_name)\n",
    "    return renamed_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN function to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/homebrew/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <E459C462-F863-3A5A-AC9F-FD77B14BE845> /opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(file_cluster_map): # function to remove duplicate files after labels made\n",
    "    cluster_files = {}\n",
    "    for file, label in file_cluster_map.items():\n",
    "        if label != -1:  # Only consider non-unique one, should find only duplicate images\n",
    "            if label not in cluster_files:\n",
    "                cluster_files[label] = []\n",
    "            cluster_files[label].append(file)\n",
    "\n",
    "    # Delete all but one file from each non-noise cluster\n",
    "    for label, files in cluster_files.items():\n",
    "        # Keep the first file and delete the rest\n",
    "        for file in files[1:]:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label device and clip preprocess method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (256,256)\n",
    "probs_end = []\n",
    "logit_text = []\n",
    "logit_image = []\n",
    "embeddings = []\n",
    "\n",
    "def create_embeddings(folder_path):\n",
    "    direc = './images/US_politics/'\n",
    " \n",
    "# target_size = (16,16)\n",
    "   \n",
    "    image_files = [os.path.join(direc, filename) for filename in os.listdir(direc) if filename.endswith('.jpg')]\n",
    "    text = clip.tokenize([\"Biden\", \"Trump\", \"Republican\", \"Democrat\"]).to(device)\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embbedding_arrays(image_files):\n",
    "    direc = './images/'+image_files\n",
    "    embeddings = []\n",
    "    target_size = (256,256)\n",
    "    # target_size = (16,16)\n",
    "    probs_end = []\n",
    "    logit_text = []\n",
    "    logit_image = []\n",
    "    extensions = ('.jpg', '.png', '.img', '.jpeg')\n",
    "    image_files = [os.path.join(direc, filename) for filename in os.listdir(direc) if filename.endswith(extensions)]\n",
    "\n",
    "    # image_files = [os.path.join(direc, filename) for filename in os.listdir(direc) if filename.endswith('.jpg')]\n",
    "    text = clip.tokenize([\"Biden\", \"Trump\", \"Republican\", \"Democrat\"]).to(device)\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        # Open and resize image\n",
    "        with Image.open(image_file) as img:\n",
    "            img_resized = img.resize(target_size)\n",
    "        \n",
    "        # Preprocess resized image\n",
    "        image = preprocess(img_resized).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text)\n",
    "            \n",
    "            logits_per_image, logits_per_text = model(image, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).numpy()  # Directly convert to NumPy array\n",
    "        \n",
    "        # Append the embedding to the list\n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "        logit_text.append(logits_per_text.cpu().numpy())\n",
    "        logit_image.append(logits_per_image.cpu().numpy())\n",
    "        probs_end.append(probs)\n",
    "\n",
    "    # Convert embeddings list to a numpy array\n",
    "    embeddings = np.array(embeddings)\n",
    "    logit_text = np.array(logit_text)\n",
    "    logit_image = np.array(logit_image)\n",
    "    probs_end = np.array(probs_end)\n",
    "    list_of_clustered_embeddings = [embeddings,logit_text,logit_image,probs_end]\n",
    "    return list_of_clustered_embeddings, image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_labels(mapsfilter, image_files):\n",
    "    embeddings_2d= mapsfilter[0]\n",
    "    dbscan = DBSCAN(eps=0.01, min_samples=2, n_jobs=-1,metric=\"cosine\")  # Compare images and label duplicates\n",
    "    labels = dbscan.fit_predict(embeddings_2d)\n",
    "    # Map each file to its corresponding cluster label\n",
    "    file_cluster_map = {file: label for file, label in zip(image_files, labels)}\n",
    "    remove_duplicates(file_cluster_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add types of photos you want to download to search array (ex: 'weapons' , 'kkk', 'trump') and it will create a new folder called 'weapons'/'kkk' and add images to it from bing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After curating and cleaning up the images you have downloaded run following to reorder files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_array = ['test']\n",
    "\n",
    "for string in search_array:\n",
    "    download_images(string)\n",
    "    list_of_clustered_embeddings,image_files = create_embbedding_arrays(string)\n",
    "    list_of_clustered_embeddings = [np.squeeze(item) for item in list_of_clustered_embeddings]\n",
    "    cluster_labels(list_of_clustered_embeddings, image_files)\n",
    "rename = capitalize_and_underscore(search_array)\n",
    "for idx, nam in enumerate(search_array):\n",
    "\n",
    "    files_update(nam,rename[idx])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
